# Lama-Triton Performance Analysis Report

本報告詳細分析 Lama-Triton 在高並發情境下的效能表現，並與基準系統 (IOPaint) 進行量化對比。

---

## 1. 測試環境 (Test Environment)
- **硬體**: AMD Ryzen 7 5800X (8 vCPU), NVIDIA RTX 3060 (12GB), Local Ubuntu Machine
- **負載模型**: 6 個並發 Worker (模擬生產環境 PHP Worker 數量)
- **測試數據**: 採用 `masking` 高解析度真實影像 (Fixed set)
- **技術指標**: P50/P95 延遲、整機 CPU 負載、任務成功率

---

## 2. 核心效能對比 (Summary Metrics)

| 指標 (Latency ms) | lama-triton (gRPC) | IOPaint (Baseline) | 加速比 / 改善率 |
| :--- | :---: | :---: | :---: |
| **P50 Latency** | **1,540 ms** | 3,934 ms | **🏆 2.55x Faster** |
| **P95 Latency** | **1,575 ms** | 4,030 ms | **2.55x Faster** |
| **Mean Latency** | **1,347 ms** | 3,874 ms | **2.87x Faster** |
| **CPU 平均百分比** | **7.5 %** | 32.2 % | **📉 降低 76%** |
| **成功率** | 100% (204 pics) | 100% (204 pics) | - |

> [!NOTE]
> 數據顯示 `lama-triton` 在處理完全相同的任務流時，不僅大幅縮短了用戶等待時間，更釋放了大量的 CPU 資源。

---

## 3. 並發處理與動態合批 (Dynamic Batching)

### 3.1 傳統處理模式 (IOPaint)
IOPaint 受限於其內部的併發管控機制（如 Global Lock），在多個 Worker 同時發起請求時，請求會形成佇列式的「序列化處理」：
- **GPU 狀態**: 長時間處於低利用率，因為每個請求都在等待資料傳輸與前處理。
- **延遲疊加**: 6 個請求同時到達，第 6 個請求必須等待前 5 個完成，導致總體 Wall-clock 時間拉長至 ~4 秒。

### 3.2 Lama-Triton 優化模式
透過 Triton 的 **Dynamic Batching** 策略，系統能自動在極短窗口 (50ms) 內收集請求：
- **Batch 推理**: 將 6 個請求合併成一次 `[6, 3, H, W]` 的 Tensor 運算。
- **平行化傳輸**: CUDA 內存拷貝與計算重疊，消除等待空檔。
- **結果**: 總體 Wall-clock 時間縮短至 ~2 秒，效能瓶頸由「前端排隊」轉移至「硬體運算限度」。

---

## 4. 系統資源分佈 (Resource Distribution)

### 4.1 CPU 消耗分析
- **IOPaint**: ~33% (Mean)。其多線程架構在處理影像編解碼時佔用較多 CPU 循環。
- **Lama-Triton**: ~8% (Mean)。Gateway 採用全異步 I/O，僅負責數據轉發，將運算壓力集中於 GPU。

### 4.2 記憶體與顯存
- **VRAM**: LaMa PyTorch Instance 佔用穩定，即使在動態排隊下也能保持預測性，不會發生記憶體溢出。

---

## 5. Flame Graph 分析結論

### 5.1 Gateway 熱點
透過 `py-spy` 分析顯示，Gateway 的主要時間花費在 **Triton gRPC Client 的等待** 與 **影像解碼 (decode_base64)**。由於這兩者均已遷移至背景線程 (`asyncio.to_thread`)，Event Loop 始終保持流暢，無阻塞現象。

### 5.2 IOPaint 熱點
IOPaint 的 Flame Graph 顯示其核心瓶頸出現在 **Model Forwarding 的鎖競爭**。這驗證了我們改採 Triton 獨立推理引擎的架構決策是正確的。

---
*Generated by Performance Test Suite*
